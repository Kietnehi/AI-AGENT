"""FastAPI Backend for AI Agent"""
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import Optional, List
import os
import shutil
from pathlib import Path

from google import genai
from google.genai import types
from tools.web_search import search_web
from tools.wolfram_tool import wolfram_compute
from tools.data_analysis import DataAnalysisTool
from config import Config

# Initialize FastAPI app
app = FastAPI(title="AI Agent API", version="1.0.0")

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Gemini client
Config.validate()
client = genai.Client(api_key=Config.GEMINI_API_KEY)

# Global data analysis tool
data_tool = DataAnalysisTool()

# Create directories
UPLOAD_DIR = Path("uploads")
CHARTS_DIR = Path("charts")
UPLOAD_DIR.mkdir(exist_ok=True)
CHARTS_DIR.mkdir(exist_ok=True)


# Request/Response Models
class ChatRequest(BaseModel):
    message: str
    feature: str  # "search", "math", "data_analysis"
    search_engine: Optional[str] = "duckduckgo"  # "duckduckgo" or "serpapi"


class ChatResponse(BaseModel):
    response: str
    status: str


class SearchRequest(BaseModel):
    query: str
    search_engine: str = "duckduckgo"
    max_results: int = 5


class MathRequest(BaseModel):
    query: str


class DataAnalysisRequest(BaseModel):
    action: str  # "summary", "info", "analyze_column", "create_chart", "ai_analyze"
    column: Optional[str] = None
    chart_type: Optional[str] = None
    x_col: Optional[str] = None
    y_col: Optional[str] = None
    title: Optional[str] = None
    prompt: Optional[str] = None  # For AI analysis


class SmartChatRequest(BaseModel):
    message: str
    search_engine: str = "google"  # "duckduckgo", "serpapi", "google"


class SmartChatResponse(BaseModel):
    response: str
    status: str
    search_performed: bool
    search_engine: Optional[str] = None


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "AI Agent API",
        "version": "1.0.0",
        "features": ["search", "math", "data_analysis"]
    }


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "gemini_configured": bool(Config.GEMINI_API_KEY),
        "wolfram_configured": bool(Config.WOLFRAM_APP_ID),
        "serpapi_configured": bool(Config.SERPAPI_KEY)
    }


@app.post("/chat")
async def chat(request: ChatRequest):
    """Main chat endpoint that routes to different features"""
    try:
        if request.feature == "search":
            # Web search - tr·∫£ v·ªÅ k·∫øt qu·∫£ thu·∫ßn t√∫y kh√¥ng qua Gemini
            results = search_web(
                request.message,
                search_engine=request.search_engine,
                max_results=5
            )
            
            return ChatResponse(
                response=results,
                status="success"
            )
        
        elif request.feature == "math":
            # Wolfram computation only - no Gemini
            wolfram_result = wolfram_compute(request.message)
            
            # Format result for text display while keeping full data for frontend
            if isinstance(wolfram_result, dict):
                formatted_text = ""
                if wolfram_result.get('text_results'):
                    formatted_text = "\n".join(wolfram_result['text_results'])
                
                if wolfram_result.get('plots'):
                    if formatted_text:
                        formatted_text += "\n\n"
                    formatted_text += f"üìä ƒê√£ t·∫°o {len(wolfram_result['plots'])} bi·ªÉu ƒë·ªì"
                
                if wolfram_result.get('images'):
                    if formatted_text:
                        formatted_text += "\n\n"
                    formatted_text += f"üñºÔ∏è ƒê√£ t·∫°o {len(wolfram_result['images'])} h√¨nh ·∫£nh"
                
                # Return the full result object as JSON string so frontend can parse it
                import json
                return ChatResponse(
                    response=json.dumps(wolfram_result),
                    status="success"
                )
            else:
                return ChatResponse(
                    response=str(wolfram_result),
                    status="success"
                )
        
        else:
            # General AI response
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=request.message
            )
            
            return ChatResponse(
                response=response.text,
                status="success"
            )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/smart-chat", response_model=SmartChatResponse)
async def smart_chat(request: SmartChatRequest):
    """
    Smart chat endpoint that automatically decides when to search for information
    AI will analyze the query and determine if web search is needed
    """
    try:
        # First, ask AI if search is needed
        decision_prompt = f"""Ph√¢n t√≠ch c√¢u h·ªèi sau v√† quy·∫øt ƒë·ªãnh xem c√≥ c·∫ßn t√¨m ki·∫øm th√¥ng tin tr√™n web kh√¥ng.

C√¢u h·ªèi: {request.message}

Tr·∫£ l·ªùi CH√çNH X√ÅC theo format JSON sau (kh√¥ng th√™m text n√†o kh√°c):
{{"need_search": true/false, "reason": "l√Ω do ng·∫Øn g·ªçn"}}

C·∫ßn t√¨m ki·∫øm (need_search: true) khi:
- C√¢u h·ªèi v·ªÅ tin t·ª©c, s·ª± ki·ªán hi·ªán t·∫°i, gi√° c·∫£ th·ªã tr∆∞·ªùng
- Th√¥ng tin c·∫≠p nh·∫≠t (th·ªùi ti·∫øt, gi√° v√†ng, gi√° bitcoin, ch·ª©ng kho√°n)
- S·ª± ki·ªán, tin t·ª©c m·ªõi, xu h∆∞·ªõng
- Th√¥ng tin c·ª• th·ªÉ v·ªÅ s·∫£n ph·∫©m, ƒë·ªãa ƒëi·ªÉm, ng∆∞·ªùi n·ªïi ti·∫øng

KH√îNG c·∫ßn t√¨m ki·∫øm (need_search: false) khi:
- C√¢u h·ªèi v·ªÅ ki·∫øn th·ª©c chung, ƒë·ªãnh nghƒ©a
- T√≠nh to√°n to√°n h·ªçc
- C√¢u h·ªèi mang t√≠nh tri·∫øt l√Ω, √Ω ki·∫øn c√° nh√¢n
- L·ªùi khuy√™n chung kh√¥ng c·∫ßn d·ªØ li·ªáu c·ª• th·ªÉ"""

        decision_response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=decision_prompt
        )
        
        # Parse decision
        import json
        decision_text = decision_response.text.strip()
        
        # Extract JSON from response (handle markdown code blocks)
        if "```json" in decision_text:
            decision_text = decision_text.split("```json")[1].split("```")[0].strip()
        elif "```" in decision_text:
            decision_text = decision_text.split("```")[1].split("```")[0].strip()
        
        try:
            decision = json.loads(decision_text)
            need_search = decision.get("need_search", False)
        except:
            # Fallback: check for keywords
            search_keywords = ["tin t·ª©c", "hi·ªán t·∫°i", "h√¥m nay", "gi√°", "c·∫≠p nh·∫≠t", "m·ªõi nh·∫•t", "th·ªùi ti·∫øt"]
            need_search = any(keyword in request.message.lower() for keyword in search_keywords)
        
        search_performed = False
        search_engine_used = None
        
        if need_search:
            # Use the search engine specified by user
            search_engine = request.search_engine
            search_engine_used = search_engine
            
            if search_engine == "google":
                # Use Gemini with Google Search grounding
                search_performed = True
                
                grounding_tool = types.Tool(
                    google_search=types.GoogleSearch()
                )
                
                config = types.GenerateContentConfig(
                    tools=[grounding_tool]
                )
                
                final_response = client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=request.message,
                    config=config
                )
                
                return SmartChatResponse(
                    response=final_response.text,
                    status="success",
                    search_performed=search_performed,
                    search_engine=search_engine_used
                )
            else:
                # Use DuckDuckGo or SerpAPI
                search_results = search_web(
                    request.message,
                    search_engine=search_engine,
                    max_results=5
                )
                search_performed = True
                
                # Generate answer based on search results
                answer_prompt = f"""D·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt v√† h·ªØu √≠ch.

C√¢u h·ªèi: {request.message}

K·∫øt qu·∫£ t√¨m ki·∫øm:
{search_results}

H√£y:
1. T·ªïng h·ª£p th√¥ng tin t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm
2. Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi
3. ƒê∆∞a ra l·ªùi khuy√™n, ph√¢n t√≠ch n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
4. S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng Markdown ƒë·ªÉ d·ªÖ ƒë·ªçc
5. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát

Tr·∫£ l·ªùi:"""
                
                # Generate final answer for DuckDuckGo/SerpAPI
                final_response = client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=answer_prompt
                )
                
                return SmartChatResponse(
                    response=final_response.text,
                    status="success",
                    search_performed=search_performed,
                    search_engine=search_engine_used
                )
            
        else:
            # Answer without search
            answer_prompt = f"""Tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch chi ti·∫øt v√† h·ªØu √≠ch:

{request.message}

H√£y:
1. Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n
2. S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng Markdown
3. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát

Tr·∫£ l·ªùi:"""
        
        # Generate final answer
        final_response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=answer_prompt
        )
        
        return SmartChatResponse(
            response=final_response.text,
            status="success",
            search_performed=search_performed,
            search_engine=search_engine_used
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/search")
async def web_search(request: SearchRequest):
    """Web search endpoint"""
    try:
        results = search_web(
            request.query,
            search_engine=request.search_engine,
            max_results=request.max_results
        )
        return {"results": results, "status": "success"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/math")
async def math_compute(request: MathRequest):
    """Wolfram Alpha computation endpoint"""
    try:
        result = wolfram_compute(request.query)
        return {
            "result": result, 
            "status": "success",
            "text_results": result.get('text_results', []),
            "images": result.get('images', []),
            "plots": result.get('plots', []),
            "success": result.get('success', False)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/upload-csv")
async def upload_csv(file: UploadFile = File(...)):
    """Upload CSV file for analysis"""
    try:
        # Save uploaded file
        file_path = UPLOAD_DIR / file.filename
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Load and analyze
        summary = data_tool.load_csv(str(file_path))
        
        return {
            "message": "File uploaded successfully",
            "filename": file.filename,
            "summary": summary,
            "columns": data_tool.df.columns.tolist() if data_tool.df is not None else [],
            "status": "success"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/analyze-data")
async def analyze_data(request: DataAnalysisRequest):
    """Analyze uploaded CSV data"""
    try:
        if data_tool.df is None:
            raise HTTPException(status_code=400, detail="No CSV file loaded")
        
        if request.action == "summary":
            result = data_tool.get_summary()
        elif request.action == "info":
            result = data_tool.get_info()
        elif request.action == "analyze_column":
            if not request.column:
                raise HTTPException(status_code=400, detail="Column name required")
            result = data_tool.analyze_column(request.column)
        elif request.action == "ai_analyze":
            if not request.prompt:
                raise HTTPException(status_code=400, detail="Prompt required for AI analysis")
            result = data_tool.analyze_with_ai(request.prompt)
        elif request.action == "create_chart":
            import time
            # Use timestamp to create unique filename
            output_filename = f"chart_{request.chart_type or 'bar'}_{int(time.time())}"
            result = data_tool.create_chart(
                chart_type=request.chart_type or "bar",
                x_col=request.x_col,
                y_col=request.y_col,
                title=request.title,
                output_file=output_filename
            )
        else:
            raise HTTPException(status_code=400, detail="Invalid action")
        
        return {"result": result, "status": "success"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/charts/{filename}")
async def get_chart(filename: str):
    """Get generated chart image"""
    file_path = CHARTS_DIR / filename
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Chart not found")
    return FileResponse(file_path)


@app.get("/charts")
async def list_charts():
    """List all generated charts"""
    charts = [f.name for f in CHARTS_DIR.glob("*.png")]
    return {"charts": charts, "status": "success"}


@app.delete("/clear-data")
async def clear_data():
    """Clear uploaded data"""
    global data_tool
    data_tool = DataAnalysisTool()
    return {"message": "Data cleared", "status": "success"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",  # Import string thay v√¨ object ƒë·ªÉ h·ªó tr·ª£ reload
        host="0.0.0.0", 
        port=8000,
        reload=True  # Auto-reload khi c√≥ thay ƒë·ªïi code
    )
