# H∆∞·ªõng D·∫´n C√†i ƒê·∫∑t v√† S·ª≠ D·ª•ng C√°c T√≠nh NƒÉng M·ªõi

## üéâ C√°c T√≠nh NƒÉng ƒê√£ Th√™m

### 1. üîä Text-to-Speech (ƒê·ªçc K·∫øt Qu·∫£ AI)
- Th√™m n√∫t **"Nghe"** ·ªü m·ªói ph·∫£n h·ªìi c·ªßa AI trong Smart Chat
- S·ª≠ d·ª•ng gTTS (Google Text-to-Speech) ƒë·ªÉ chuy·ªÉn vƒÉn b·∫£n th√†nh gi·ªçng n√≥i
- H·ªó tr·ª£ ti·∫øng Vi·ªát

### 2. üñ•Ô∏è Local LLM v·ªõi Qwen 1.5B
- Cho ph√©p ng∆∞·ªùi d√πng ch·∫°y m√¥ h√¨nh AI local tr√™n m√°y
- Kh√¥ng c·∫ßn internet sau khi t·∫£i model
- S·ª≠ d·ª•ng Qwen 2.5-1.5B-Instruct (phi√™n b·∫£n nh·∫π h∆°n)
- T·ª± ƒë·ªông detect GPU/CPU

### 3. üëÅÔ∏è Vision AI (VQA - Visual Question Answering)
- H·ªèi ƒë√°p v·ªÅ n·ªôi dung h√¨nh ·∫£nh
- S·ª≠ d·ª•ng model Llava-Phi2
- Upload ·∫£nh v√† ƒë·∫∑t c√¢u h·ªèi b·∫±ng ti·∫øng Anh

### 4. üìÑ OCR (Tr√≠ch Xu·∫•t VƒÉn B·∫£n t·ª´ ·∫¢nh)
- Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ h√¨nh ·∫£nh
- S·ª≠ d·ª•ng DeepSeek-OCR model
- H·ªó tr·ª£ nhi·ªÅu lo·∫°i h√¨nh ·∫£nh

## üì¶ C√†i ƒê·∫∑t

### Backend

1. **C·∫≠p nh·∫≠t dependencies:**
```bash
cd backend
pip install -r requirements.txt
```

**L∆∞u √Ω:** C√°c models s·∫Ω t·ª± ƒë·ªông t·∫£i v·ªÅ khi s·ª≠ d·ª•ng l·∫ßn ƒë·∫ßu:
- Qwen 2.5-1.5B-Instruct (~3GB)
- Llava-Phi2 cho VQA (~6GB)
- DeepSeek-OCR cho OCR (~2GB)

2. **Kh·ªüi ƒë·ªông backend:**
```bash
python main.py
```

### Frontend

Kh√¥ng c·∫ßn c√†i th√™m g√¨, frontend ƒë√£ c√≥ s·∫µn c√°c component m·ªõi.

## üöÄ S·ª≠ D·ª•ng

### Text-to-Speech
1. M·ªü **Smart Chat**
2. G·ª≠i tin nh·∫Øn v√† nh·∫≠n ph·∫£n h·ªìi t·ª´ AI
3. Click n√∫t **"Nghe"** ƒë·ªÉ nghe AI ƒë·ªçc k·∫øt qu·∫£
4. Audio s·∫Ω t·ª± ƒë·ªông ph√°t

### Local LLM (Qwen 1.5B)
1. Click v√†o tab **"Local LLM"** trong menu
2. Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n
3. Click **"G·ª≠i"**
4. L·∫ßn ƒë·∫ßu s·∫Ω t·∫£i model (m·∫•t v√†i ph√∫t)
5. Sau ƒë√≥ AI s·∫Ω tr·∫£ l·ªùi t·ª´ model local

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Ch·∫°y offline (sau khi t·∫£i model)
- ‚úÖ B·∫£o m·∫≠t (d·ªØ li·ªáu kh√¥ng g·ª≠i l√™n server)
- ‚úÖ Mi·ªÖn ph√≠ (kh√¥ng t·ªën API)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è C·∫ßn RAM/VRAM (khuy·∫øn ngh·ªã >= 8GB)
- ‚ö†Ô∏è Ch·∫≠m h∆°n cloud API
- ‚ö†Ô∏è Model nh·ªè h∆°n c√≥ th·ªÉ k√©m th√¥ng minh h∆°n

### Vision AI (VQA)
1. Click v√†o tab **"Vision AI"**
2. Ch·ªçn **"H·ªèi ƒê√°p V·ªÅ ·∫¢nh (VQA)"**
3. Click **"Ch·ªçn ·∫¢nh"** v√† ch·ªçn file
4. Click **"T·∫£i L√™n"**
5. Nh·∫≠p c√¢u h·ªèi b·∫±ng ti·∫øng Anh (v√≠ d·ª•: "What is in this image?")
6. Click **"Tr·∫£ L·ªùi C√¢u H·ªèi"**
7. L·∫ßn ƒë·∫ßu s·∫Ω t·∫£i model (m·∫•t v√†i ph√∫t)
8. AI s·∫Ω tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ ·∫£nh

**V√≠ d·ª• c√¢u h·ªèi:**
- "What objects are in this image?"
- "Describe this image"
- "What color is the car?"
- "How many people are there?"

### OCR (Tr√≠ch Xu·∫•t VƒÉn B·∫£n)
1. Click v√†o tab **"Vision AI"**
2. Ch·ªçn **"Tr√≠ch Xu·∫•t VƒÉn B·∫£n (OCR)"**
3. Click **"Ch·ªçn ·∫¢nh"** v√† ch·ªçn file ·∫£nh c√≥ ch·ªØ
4. Click **"T·∫£i L√™n"**
5. Click **"Tr√≠ch Xu·∫•t VƒÉn B·∫£n"**
6. L·∫ßn ƒë·∫ßu s·∫Ω t·∫£i model (m·∫•t v√†i ph√∫t)
7. VƒÉn b·∫£n s·∫Ω ƒë∆∞·ª£c tr√≠ch xu·∫•t v√† hi·ªÉn th·ªã

**Th√≠ch h·ª£p cho:**
- ·∫¢nh ch·ª•p t√†i li·ªáu
- Screenshot c√≥ text
- H√≥a ƒë∆°n, bi·ªÉn b√°o
- B·∫•t k·ª≥ ·∫£nh n√†o c√≥ ch·ªØ

## ‚öôÔ∏è Y√™u C·∫ßu H·ªá Th·ªëng

### T·ªëi Thi·ªÉu:
- RAM: 8GB
- Disk: 15GB tr·ªëng (cho models)
- CPU: Intel i5 ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng

### Khuy·∫øn Ngh·ªã:
- RAM: 16GB+
- GPU: NVIDIA v·ªõi 6GB+ VRAM (s·ª≠ d·ª•ng CUDA)
- Disk: SSD v·ªõi 20GB+ tr·ªëng
- CPU: Intel i7 ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng

### Ki·ªÉm Tra GPU Support:
```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}")
```

## üîß C·∫•u H√¨nh

### Thay ƒê·ªïi Model LLM

Trong `backend/tools/local_llm.py`, b·∫°n c√≥ th·ªÉ thay ƒë·ªïi model:

```python
# M·∫∑c ƒë·ªãnh (nh·∫π, nhanh)
local_llm = LocalLLM(model_name="Qwen/Qwen2.5-1.5B-Instruct")

# Ho·∫∑c d√πng 3B (th√¥ng minh h∆°n nh∆∞ng n·∫∑ng h∆°n)
local_llm = LocalLLM(model_name="Qwen/Qwen2.5-3B-Instruct")

# Ho·∫∑c 7B (r·∫•t th√¥ng minh nh∆∞ng c·∫ßn GPU m·∫°nh)
local_llm = LocalLLM(model_name="Qwen/Qwen2.5-7B-Instruct")
```

### Thay ƒê·ªïi Voice Language

Trong `backend/main.py`, endpoint `/text-to-speech`:

```python
# Ti·∫øng Vi·ªát (m·∫∑c ƒë·ªãnh)
tts = gTTS(text=request.text, lang='vi')

# Ti·∫øng Anh
tts = gTTS(text=request.text, lang='en')
```

## üìù API Endpoints M·ªõi

### 1. Text-to-Speech
```
POST /text-to-speech
Body: {
  "text": "VƒÉn b·∫£n c·∫ßn ƒë·ªçc",
  "lang": "vi"
}
Response: Audio file (MP3)
```

### 2. Local LLM
```
POST /local-llm
Body: {
  "message": "Tin nh·∫Øn c·ªßa b·∫°n",
  "max_length": 512,
  "temperature": 0.7
}
Response: {
  "response": "Ph·∫£n h·ªìi t·ª´ AI",
  "model": "Qwen/Qwen2.5-1.5B-Instruct",
  "device": "cuda/cpu",
  "status": "success"
}
```

### 3. Upload Image
```
POST /upload-image
Body: FormData with image file
Response: {
  "message": "Image uploaded successfully",
  "filename": "image.jpg",
  "status": "success"
}
```

### 4. Vision Analysis
```
POST /vision
Body: {
  "action": "vqa",  // or "ocr"
  "image_filename": "image.jpg",
  "question": "What is this?"  // for VQA only
}
Response: {
  "result": {
    "success": true,
    "answer": "...",  // for VQA
    "text": "..."     // for OCR
  },
  "status": "success"
}
```

## üêõ Troubleshooting

### Model t·∫£i ch·∫≠m
- Models t·∫£i t·ª´ Hugging Face, c·∫ßn internet t·ªëc ƒë·ªô cao
- L·∫ßn ƒë·∫ßu t·∫£i s·∫Ω l√¢u (c√≥ th·ªÉ 10-30 ph√∫t)
- C√°c l·∫ßn sau s·∫Ω d√πng cache, nhanh h∆°n

### Out of Memory (OOM)
- Gi·∫£m `max_length` trong Local LLM
- ƒê√≥ng c√°c ·ª©ng d·ª•ng kh√°c ƒëang ch·∫°y
- D√πng model nh·ªè h∆°n (1.5B thay v√¨ 3B/7B)
- Ch·∫°y tr√™n CPU n·∫øu GPU kh√¥ng ƒë·ªß

### L·ªói CUDA
```bash
# C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Model kh√¥ng load ƒë∆∞·ª£c
```bash
# Clear cache v√† t·∫£i l·∫°i
rm -rf ~/.cache/huggingface/
python -c "from transformers import AutoModel; AutoModel.from_pretrained('model_name')"
```

## üìä So S√°nh Models

| Feature | API (Gemini) | Local (Qwen 1.5B) |
|---------|--------------|-------------------|
| T·ªëc ƒë·ªô | ‚ö°‚ö°‚ö° Nhanh | üêå Ch·∫≠m |
| Offline | ‚ùå C·∫ßn internet | ‚úÖ Kh√¥ng c·∫ßn |
| Chi ph√≠ | üí∞ T·ªën API | ‚úÖ Mi·ªÖn ph√≠ |
| B·∫£o m·∫≠t | ‚ö†Ô∏è G·ª≠i l√™n server | ‚úÖ Local |
| Th√¥ng minh | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Y√™u c·∫ßu | üîå Internet | üíª RAM/GPU |

## üéØ Use Cases

### Smart Chat + TTS
- Tr·ª£ l√Ω ·∫£o c√≥ gi·ªçng n√≥i
- ƒê·ªçc tin t·ª©c, b√°o c√°o
- H·ªçc ngo·∫°i ng·ªØ

### Local LLM
- X·ª≠ l√Ω d·ªØ li·ªáu nh·∫°y c·∫£m
- M√¥i tr∆∞·ªùng kh√¥ng c√≥ internet
- Ti·∫øt ki·ªám chi ph√≠ API

### Vision AI (VQA)
- Ph√¢n t√≠ch ·∫£nh s·∫£n ph·∫©m
- H·ªó tr·ª£ ng∆∞·ªùi khi·∫øm th·ªã
- Ki·ªÉm tra ch·∫•t l∆∞·ª£ng ·∫£nh

### OCR
- S·ªë h√≥a t√†i li·ªáu gi·∫•y
- ƒê·ªçc h√≥a ƒë∆°n, bi√™n lai
- Tr√≠ch xu·∫•t text t·ª´ screenshot

## üìö Resources

- [Qwen Models](https://huggingface.co/Qwen)
- [Llava-Phi2](https://huggingface.co/Navyabhat/Llava-Phi2)
- [DeepSeek-OCR](https://huggingface.co/deepseek-ai/DeepSeek-OCR)
- [gTTS Documentation](https://gtts.readthedocs.io/)

## ü§ù Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, h√£y ki·ªÉm tra:
1. Log c·ªßa backend (terminal ch·∫°y `python main.py`)
2. Console c·ªßa browser (F12)
3. Requirements ƒë√£ c√†i ƒë·ªß ch∆∞a
4. System c√≥ ƒë·ªß RAM/GPU ch∆∞a

---

**Ch√∫c b·∫°n s·ª≠ d·ª•ng vui v·∫ª! üéâ**
